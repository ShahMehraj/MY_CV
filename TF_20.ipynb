{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/cQmdDnkFfybhcmdrJxkl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShahMehraj/MY_CV/blob/main/TF_20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate the data set:"
      ],
      "metadata": {
        "id": "SbEdm2dbaVZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set the random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the number of training examples and test examples\n",
        "m_train = 1000\n",
        "m_test = 100\n",
        "\n",
        "# Generate the training set inputs\n",
        "X_train_1 = np.random.randn(m_train, 1) # random values for x_1\n",
        "X_train_2 = np.random.randn(m_train, 1) # random values for x_2\n",
        "\n",
        "# Generate the training set targets\n",
        "y_train = X_train_1 ** 2 + X_train_2 + 1\n",
        "\n",
        "# Combine the training set inputs into a single matrix\n",
        "X_train = np.hstack((X_train_1, X_train_2))\n",
        "\n",
        "# Generate the test set inputs\n",
        "X_test_1 = np.random.randn(m_test, 1) # random values for x_1\n",
        "X_test_2 = np.random.randn(m_test, 1) # random values for x_2\n",
        "\n",
        "# Generate the test set targets\n",
        "y_test = X_test_1 ** 2 + X_test_2 + 1\n",
        "\n",
        "# Combine the test set inputs into a single matrix\n",
        "X_test = np.hstack((X_test_1, X_test_2))\n",
        "print(X_train_1[0]**2+X_train_2[0]+1)\n",
        "print(Y_train[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpxow7ljaR29",
        "outputId": "cb9cfec5-2e4a-4c12-f694-f12a4e84c7a9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2.64608039]\n",
            "[3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initialiaze parameters\n",
        "To train a model, we first initialize the parameters"
      ],
      "metadata": {
        "id": "pMHd5O7Zh0xx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def initialize_param(layer_dim):\n",
        "    param = {}\n",
        "    L = len(layer_dim)\n",
        "    for i in range(1, L):\n",
        "        param[\"W\" + str(i)] = np.random.randn(layer_dim[i], layer_dim[i-1]) * 0.01\n",
        "        param[\"b\" + str(i)] = np.zeros((layer_dim[i], 1))\n",
        "    return param\n"
      ],
      "metadata": {
        "id": "WqA_nrqphvvl"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Forward Propagation\n",
        "Now we compute the output for the given training example using the folloiwng function"
      ],
      "metadata": {
        "id": "av1HIjRLJTir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_prop(X, param):\n",
        "    cache = {}\n",
        "    L = len(param) // 2\n",
        "    A_prev = X\n",
        "\n",
        "    for l in range(1, L):\n",
        "        Z = np.dot(A_prev, param['W' + str(l)].T) + param['b' + str(l)]\n",
        "        A = np.maximum(0, Z)\n",
        "        cache['Z' + str(l)] = Z\n",
        "        cache['A' + str(l)] = A\n",
        "        A_prev = A\n",
        "\n",
        "    ZL = np.dot(A_prev, param['W' + str(L)].T) + param['b' + str(L)]\n",
        "    Y = 1 / (1 + np.exp(-ZL))\n",
        "    cache['Z' + str(L)] = ZL\n",
        "    cache['A' + str(L)] = Y\n",
        "\n",
        "    return Y, cache\n"
      ],
      "metadata": {
        "id": "Gbqw_IdWCsQ4"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing Loss Function\n",
        "After forward propagation, we compute the loss functoin, which is:\n",
        "$$L=\\frac{1}{2}(\\hat{y}-y)^2$$"
      ],
      "metadata": {
        "id": "TpT5SrbHLnGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(X, Y_true, param):\n",
        "    m = X.shape[0]\n",
        "    Y_pred = forward_prop(X, param)\n",
        "    L = -(Y_true * np.log(Y_pred) + (1 - Y_true) * np.log(1 - Y_pred))\n",
        "    J = np.sum(L) / m\n",
        "    return J\n"
      ],
      "metadata": {
        "id": "agkzhe7HkBlg"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backward Propagation\n",
        "After computing the cost, we do backward propagation to compute the derivatives of the loss function with respect to each parameter."
      ],
      "metadata": {
        "id": "VAbBQ6yGMwfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_prop(X, Y, cache, param):\n",
        "    m = X.shape[1]\n",
        "    L = len(param) // 2\n",
        "    grads = {}\n",
        "\n",
        "    dA = - (np.divide(Y, cache['A' + str(L)]) - np.divide(1 - Y, 1 - cache['A' + str(L)]))\n",
        "\n",
        "    dZ = dA * cache['A' + str(L)] * (1 - cache['A' + str(L)])\n",
        "    grads['dA' + str(L)] = dA\n",
        "    grads['dZ' + str(L)] = dZ\n",
        "    grads['dW' + str(L)] = 1./m * np.dot(dZ, cache['A' + str(L-1)].T)\n",
        "    grads['db' + str(L)] = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "    for l in reversed(range(1, L)):\n",
        "        dA_prev = np.dot(param['W' + str(l+1)].T, dZ)\n",
        "        dZ = np.multiply(dA_prev, np.int64(cache['A' + str(l)] > 0))\n",
        "        grads['dA' + str(l)] = dA_prev\n",
        "        grads['dZ' + str(l)] = dZ\n",
        "        grads['dW' + str(l)] = 1./m * np.dot(dZ, cache['A' + str(l-1)].T)\n",
        "        grads['db' + str(l)] = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
        "\n",
        "    return grads\n"
      ],
      "metadata": {
        "id": "X1n22rDZMtyr"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Update Parameters\n",
        "Next we update parameters to minimize our cost function"
      ],
      "metadata": {
        "id": "HpVpSQiVYun2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_param(param, grads, learning_rate):\n",
        "  L=len(param)//2\n",
        "  for l in range(1, L+1):\n",
        "    param[\"W\"+str(l)]=param[\"W\"+str(l)]-learning_rate*grads[\"dW\"+str(l)]\n",
        "    param[\"b\"+str(l)]=param[\"b\"+str(l)]-learning_rate*grads[\"db\"+str(l)]\n",
        "  return param"
      ],
      "metadata": {
        "id": "dgIWKUfdUiXS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the Model\n",
        "Now use the above helper functions to create the model"
      ],
      "metadata": {
        "id": "lk8P-xLAdLJw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X_train, Y_train, learning_rate, num_iteration):\n",
        "  layer_dim=[2, 2, 1]\n",
        "  param=initialize_param(layer_dim)\n",
        "  for i in range(num_iteration):\n",
        "    Y, cache = forward_prop(X_train, param)\n",
        "    J = compute_cost(Y, Y_train, param)\n",
        "    grads = backward_prop(X_train, Y, cache, param)\n",
        "    param = update_param(param, grads, learning_rate)\n",
        "    print(\"Cost after iteration %i: %f\" %(i+1, J))\n",
        "  \n"
      ],
      "metadata": {
        "id": "6W7RB3GDZTdg"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.01\n",
        "num_iteration = 1000\n",
        "model(X_train.T, y_train.T, learning_rate, num_iteration)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "rr-VdI1ue01y",
        "outputId": "02090ece-e831-4ffe-c4cf-9edafab4e9f2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-dac49232d2a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-56-aeba822f8912>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, learning_rate, num_iteration)\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitialize_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mJ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-52-14ae5b993ea5>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(X, param)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Z'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (2,1000) and (2,2) not aligned: 1000 (dim 1) != 2 (dim 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mMUWYqlJe_b_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}